# -*- coding: utf-8 -*-
"""GenAI_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FHyNojX5D9T0Ywh-5d-uaUuqgu7-91Vq
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastLanguageModel
import torch

max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gemma-2b-bnb-4bit",  # ✅ Switch to Gemma 2B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

FastLanguageModel.for_inference(model)  # Optional: activates Unsloth speed boosts

model.config.layerdrop = 0.1  # Simulated Progressive Layer Dropping

model = FastLanguageModel.get_peft_model(
    model,
    r = 8,
    lora_alpha = 16,
    lora_dropout = 0.05,
    target_modules = ["q_proj", "v_proj"],  # ✅ Best match for Gemma
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    use_rslora = False,
    loftq_config = None,
)

from datasets import load_dataset
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from trl import SFTTrainer
from unsloth.chat_templates import get_chat_template
from unsloth import FastLanguageModel

# Load IMDB dataset
dataset = load_dataset("imdb", split="train[:20000]")  # Subset for quick training

# We'll create a 'text' field with prompt + label to match SFTTrainer expectations
STATIC_PROMPT = "Classify the sentiment of this review:\n\n"

def preprocess(example):
    review = example["text"]
    label = "positive" if example["label"] == 1 else "negative"
    prompt = f"{STATIC_PROMPT}{review}\nSentiment: {label}"
    return {"text": prompt}

dataset = dataset.map(preprocess)
dataset_text_field = "text"

STATIC_PROMPT = "Classify the sentiment of this review:\n\n"

def preprocess(example):
    review = example["text"]
    label = "positive" if example["label"] == 1 else "negative"

    # Full text for training
    full_text = f"{STATIC_PROMPT}{review}\nSentiment: {label}"

    # Tokenize full string
    tokenized = tokenizer(full_text, padding="max_length", truncation=True, max_length=512)

    # For CLM: labels must be same length as input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()

    return tokenized

dataset = dataset.map(preprocess)

from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="weighted")
    }


data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Important for CLM tasks like this
)


training_args = TrainingArguments(
    output_dir="./outputs",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    logging_steps=1,
    report_to="none",
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    eval_dataset=dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train()

model.save_pretrained("./unsloth_gemma2b_lora_adapter")
tokenizer.save_pretrained("./unsloth_gemma2b_lora_adapter")

!zip -r unsloth_gemma2b_lora_adapter.zip unsloth_gemma2b_lora_adapter

from google.colab import files
files.download("unsloth_gemma2b_lora_adapter.zip")

